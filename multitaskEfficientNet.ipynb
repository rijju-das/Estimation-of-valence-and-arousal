{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Directory and file paths\n",
    "train_dir = 'train_set/images'\n",
    "val_dir = 'val_set/images'\n",
    "train_ann_file = 'train_annotation.csv'\n",
    "val_ann_file = 'val_annotation.csv'\n",
    "\n",
    "# Parameters\n",
    "input_shape = (224, 224, 3)  # Specify input image shape\n",
    "batch_size = 24\n",
    "num_emotions = 8\n",
    "\n",
    "# Function to one-hot encode emotion labels\n",
    "def one_hot_encode(number, num_classes=num_emotions):\n",
    "    one_hot_vector = np.zeros(num_classes)\n",
    "    one_hot_vector[number] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Training parameters\n",
    "# epoch_steps = 30000 // batch_size\n",
    "epoch_steps=30\n",
    "val_steps = 30\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, GlobalAveragePooling2D, Activation, Reshape, Add\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, AvgPool2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CCC function\n",
    "def ccc(y_true, y_pred):\n",
    "    y_true_mean = tf.reduce_mean(y_true, axis=0)\n",
    "    y_pred_mean = tf.reduce_mean(y_pred, axis=0)\n",
    "    \n",
    "    covariance = tf.reduce_mean((y_true - y_true_mean) * (y_pred - y_pred_mean))\n",
    "    true_var = tf.reduce_mean((y_true - y_true_mean) ** 2)\n",
    "    pred_var = tf.reduce_mean((y_pred - y_pred_mean) ** 2)\n",
    "    \n",
    "    ccc_value = 2 * covariance / (true_var + pred_var + tf.square(y_true_mean - y_pred_mean))\n",
    "    \n",
    "    return 1 - ccc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate augmented data for minority class images\n",
    "def data_generator(dir_path, ann_file, batch_size, input_shape, augment=False):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    minority_classes=[4,5,7]\n",
    "    anno = pd.read_csv(ann_file)\n",
    "    while True:\n",
    "        batch_paths = np.random.choice(anno['filename'], size=batch_size)\n",
    "        batch_input, batch_output_valence, batch_output_arousal, batch_output_emotion = [], [], [], []\n",
    "\n",
    "        for input_path in batch_paths:\n",
    "            img_path = os.path.join(dir_path, str(input_path)+'.jpg')\n",
    "            img = load_img(img_path, target_size=input_shape)\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)  # Reshape to add batch dimension\n",
    "            img_array = next(datagen.flow(img_array, batch_size=1))[0]\n",
    "            row = anno[anno[\"filename\"] == input_path]\n",
    "            if row.empty:\n",
    "                print(f\"Warning: No annotation found for {input_path}\")\n",
    "                continue\n",
    "            \n",
    "            batch_input.append(img_array)\n",
    "            batch_output_valence.append(row[\"Valance\"].values[0])\n",
    "            batch_output_arousal.append(row[\"Arousal\"].values[0])\n",
    "            batch_output_emotion.append(one_hot_encode(row[\"Expression\"].values[0]))\n",
    "\n",
    "        batch_input = np.array(batch_input)\n",
    "        batch_output_valence = np.array(batch_output_valence)\n",
    "        batch_output_arousal = np.array(batch_output_arousal)\n",
    "        batch_output_emotion = np.array(batch_output_emotion)\n",
    "\n",
    "        yield batch_input, {\"valence\": batch_output_valence, \"arousal\": batch_output_arousal, \"emotion\": batch_output_emotion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampling majority class and upsampling minority class\n",
    "def data_generator_down_up(dir_path, ann_file, batch_size, input_shape, augment=False):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    minority_classes = [4, 5, 7]\n",
    "    ann_file = pd.read_csv(ann_file)\n",
    "    majority_classes = ann_file[~ann_file[\"Expression\"].isin(minority_classes)]\n",
    "    minority_classes = ann_file[ann_file[\"Expression\"].isin(minority_classes)]\n",
    "\n",
    "    while True:\n",
    "        batch_input, batch_output_valence, batch_output_arousal, batch_output_emotion = [], [], [], []\n",
    "\n",
    "        majority_batch_paths = np.random.choice(majority_classes['filename'], size=batch_size // 2)\n",
    "        minority_batch_paths = np.random.choice(minority_classes['filename'], size=batch_size // 2)\n",
    "        # Determine the number of samples to take from minority and majority classes\n",
    "        # minority_size = int(batch_size * 0.50)\n",
    "        # majority_size = batch_size - minority_size\n",
    "        \n",
    "        # # Randomly select samples from the minority and majority annotations\n",
    "        # minority_batch_paths = minority_classes.sample(n=minority_size, replace=True)['filename']\n",
    "        # majority_batch_paths = majority_classes.sample(n=majority_size, replace=True)['filename']\n",
    "        \n",
    "        for input_path in np.concatenate((majority_batch_paths, minority_batch_paths)):\n",
    "            img_path = os.path.join(dir_path, str(input_path)+'.jpg')\n",
    "            img = load_img(img_path, target_size=input_shape)\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)  # Reshape to add batch dimension\n",
    "            \n",
    "            row = ann_file[ann_file[\"filename\"] == input_path]\n",
    "            if row.empty:\n",
    "                print(f\"Warning: No annotation found for {input_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Check if the image belongs to a minority class\n",
    "            emotion_class = row[\"Expression\"].values[0]\n",
    "            if augment and emotion_class in minority_classes:\n",
    "                # Apply data augmentation\n",
    "                for _ in range(3):  # Generate 3 augmented images\n",
    "                    augmented_img = next(datagen.flow(img_array, batch_size=1))[0]\n",
    "                    batch_input.append(augmented_img)  # Remove batch dimension\n",
    "                    batch_output_valence.append(row[\"Valance\"].values[0])\n",
    "                    batch_output_arousal.append(row[\"Arousal\"].values[0])\n",
    "                    batch_output_emotion.append(one_hot_encode(row[\"Expression\"].values[0]))\n",
    "                    \n",
    "            else:\n",
    "                # No augmentation for majority classes\n",
    "                batch_input.append(img_array[0])\n",
    "                batch_output_valence.append(row[\"Valance\"].values[0])\n",
    "                batch_output_arousal.append(row[\"Arousal\"].values[0])\n",
    "                batch_output_emotion.append(one_hot_encode(row[\"Expression\"].values[0]))\n",
    "\n",
    "        batch_input = np.array(batch_input)\n",
    "        batch_output_valence = np.array(batch_output_valence)\n",
    "        batch_output_arousal = np.array(batch_output_arousal)\n",
    "        batch_output_emotion = np.array(batch_output_emotion)\n",
    "\n",
    "        yield batch_input, {\"valence\": batch_output_valence, \"arousal\": batch_output_arousal, \"emotion\": batch_output_emotion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generator function with augmentation and equal class distribution\n",
    "def data_generator_equal(dir_path, ann_file, batch_size, input_shape, augment=False, num_classes=8, augment_count=1):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    anno = pd.read_csv(ann_file)\n",
    "    class_counts = anno['Expression'].value_counts().to_dict()\n",
    "\n",
    "    while True:\n",
    "        # Initialize empty lists for the batch\n",
    "        batch_input, batch_output_valence, batch_output_arousal, batch_output_emotion = [], [], [], []\n",
    "        \n",
    "        # Determine the number of samples per class\n",
    "        samples_per_class = batch_size // num_classes\n",
    "        \n",
    "        for emotion_class in range(num_classes):\n",
    "            class_anno = anno[anno[\"Expression\"] == emotion_class]\n",
    "            selected_paths = class_anno.sample(n=samples_per_class, replace=True)['filename']\n",
    "            \n",
    "            for input_path in selected_paths:\n",
    "                img_path = os.path.join(dir_path, str(input_path)+'.jpg')\n",
    "                try:\n",
    "                    img = load_img(img_path, target_size=input_shape)\n",
    "                    img_array = img_to_array(img)\n",
    "                    img_array = np.expand_dims(img_array, axis=0)  # Reshape to add batch dimension\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img_path}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                row = class_anno[class_anno[\"filename\"] == input_path]\n",
    "                if row.empty:\n",
    "                    print(f\"Warning: No annotation found for {input_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Check if the image should be augmented\n",
    "                if augment:\n",
    "                    # Apply data augmentation\n",
    "                    for _ in range(augment_count):  # Generate augment_count augmented images\n",
    "                        augmented_img = next(datagen.flow(img_array, batch_size=1))[0]\n",
    "                        batch_input.append(augmented_img)  # Remove batch dimension\n",
    "                        batch_output_valence.append(row[\"Valance\"].values[0])\n",
    "                        batch_output_arousal.append(row[\"Arousal\"].values[0])\n",
    "                        batch_output_emotion.append(one_hot_encode(row[\"Expression\"].values[0], num_classes=num_classes))\n",
    "                else:\n",
    "                    # No augmentation\n",
    "                    batch_input.append(img_array[0])\n",
    "                    batch_output_valence.append(row[\"Valance\"].values[0])\n",
    "                    batch_output_arousal.append(row[\"Arousal\"].values[0])\n",
    "                    batch_output_emotion.append(one_hot_encode(row[\"Expression\"].values[0], num_classes=num_classes))\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        try:\n",
    "            batch_input = np.array(batch_input)\n",
    "            batch_output_valence = np.array(batch_output_valence)\n",
    "            batch_output_arousal = np.array(batch_output_arousal)\n",
    "            batch_output_emotion = np.array(batch_output_emotion)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting batch to numpy arrays: {e}\")\n",
    "            continue\n",
    "        yield batch_input, {\"valence\": batch_output_valence, \"arousal\": batch_output_arousal, \"emotion\": batch_output_emotion}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definitioin: AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def swish(x):\n",
    "    return x * tf.nn.sigmoid(x)\n",
    "\n",
    "def SEBlock(inputs, reduction_ratio=4):\n",
    "    channels = inputs.shape[-1]\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Dense(channels // reduction_ratio, activation='relu')(x)\n",
    "    x = Dense(channels, activation='sigmoid')(x)\n",
    "    x = Reshape((1, 1, channels))(x)\n",
    "    return inputs * x\n",
    "\n",
    "def MBConvBlock(inputs, expand_ratio, output_channels, stride):\n",
    "    input_channels = inputs.shape[-1]\n",
    "    expanded_channels = input_channels * expand_ratio\n",
    "    \n",
    "    x = Conv2D(expanded_channels, kernel_size=1, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(swish)(x)\n",
    "    \n",
    "    x = DepthwiseConv2D(kernel_size=3, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(swish)(x)\n",
    "    \n",
    "    x = SEBlock(x)\n",
    "    \n",
    "    x = Conv2D(output_channels, kernel_size=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    if input_channels == output_channels and stride == 1:\n",
    "        x = Add()([x, inputs])\n",
    "    \n",
    "    return x\n",
    "\n",
    "def EfficientNet(input_shape, num_classes, alpha=1.0, beta=1.0):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # Stem\n",
    "    x = Conv2D(int(32 * alpha), kernel_size=3, strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(swish)(x)\n",
    "    \n",
    "    # Blocks\n",
    "    block_settings = [\n",
    "        {'num_repeat': 1, 'expand_ratio': 1, 'output_channels': 16, 'stride': 1},\n",
    "        {'num_repeat': 2, 'expand_ratio': 6, 'output_channels': 24, 'stride': 2},\n",
    "        {'num_repeat': 2, 'expand_ratio': 6, 'output_channels': 40, 'stride': 2},\n",
    "        {'num_repeat': 3, 'expand_ratio': 6, 'output_channels': 80, 'stride': 2},\n",
    "        {'num_repeat': 3, 'expand_ratio': 6, 'output_channels': 112, 'stride': 1},\n",
    "        {'num_repeat': 4, 'expand_ratio': 6, 'output_channels': 192, 'stride': 2},\n",
    "        {'num_repeat': 1, 'expand_ratio': 6, 'output_channels': 320, 'stride': 1}\n",
    "    ]\n",
    "    \n",
    "    for settings in block_settings:\n",
    "        num_repeat = settings['num_repeat']\n",
    "        expand_ratio = settings['expand_ratio']\n",
    "        output_channels = int(settings['output_channels'] * alpha)\n",
    "        stride = settings['stride']\n",
    "        \n",
    "        for _ in range(num_repeat):\n",
    "            x = MBConvBlock(x, expand_ratio, output_channels, stride)\n",
    "            stride = 1\n",
    "    \n",
    "    # Head\n",
    "    x = Conv2D(int(1280 * alpha), kernel_size=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(swish)(x)\n",
    "    \n",
    "    # Squeeze-and-Excitation\n",
    "    x = SEBlock(x, reduction_ratio=beta)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Output for each task\n",
    "    outputs = {\n",
    "        'valence': Dense(1, activation='linear', name='valence')(x),\n",
    "        'arousal': Dense(1, activation='linear', name='arousal')(x),\n",
    "        'emotion': Dense(num_classes, activation='softmax', name='emotion')(x)\n",
    "    }\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "modelOri = EfficientNet(input_shape, num_emotions)\n",
    "modelOri.compile(optimizer=Adam(),\n",
    "              loss={'valence': 'mse', 'arousal': 'mse', 'emotion': 'categorical_crossentropy'},\n",
    "              metrics={'valence': ['mae','accuracy',ccc,rmse], 'arousal': ['mae','accuracy', ccc, rmse], 'emotion': ['accuracy']})\n",
    "\n",
    "batch_size = 5  # Reduce the batch size\n",
    "epoch_steps = 10  # Adjust according to your dataset\n",
    "epoch = 5  # Number of epochs\n",
    "\n",
    "# Training and validation data generators\n",
    "train_genOri = data_generator(train_dir, train_ann_file, batch_size, input_shape)\n",
    "val_genOri = data_generator(val_dir, val_ann_file, batch_size, input_shape)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='valence_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_modelOri.h5', save_best_only=True, monitor='valence_loss'),\n",
    "    ReduceLROnPlateau(monitor='valence_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "historyOri = modelOri.fit(train_genOri, steps_per_epoch=epoch_steps, epochs=epoch, callbacks=callbacks, validation_data=val_genOri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[24,144,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/depthwise_conv2d_3/depthwise (defined at C:\\Users\\rijju\\AppData\\Local\\Temp\\ipykernel_4052\\2767804707.py:19) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_13012]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/depthwise_conv2d_3/depthwise:\n model/activation_7/mul (defined at C:\\Users\\rijju\\AppData\\Local\\Temp\\ipykernel_4052\\821000116.py:2)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     12\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalence_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     14\u001b[0m     ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_modelOri.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalence_loss\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     15\u001b[0m     ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalence_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m     16\u001b[0m ]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m historyOri \u001b[38;5;241m=\u001b[39m \u001b[43mmodelOri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_genOri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1183\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1177\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1178\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1179\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1180\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1181\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1182\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1183\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1184\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1185\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:889\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 889\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    891\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    892\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:950\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    947\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m   _, _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    953\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    954\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:3023\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3021\u001b[0m   (graph_function,\n\u001b[0;32m   3022\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1960\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1956\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1959\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1962\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m     args,\n\u001b[0;32m   1964\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1965\u001b[0m     executing_eagerly)\n\u001b[0;32m   1966\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[24,144,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/depthwise_conv2d_3/depthwise (defined at C:\\Users\\rijju\\AppData\\Local\\Temp\\ipykernel_4052\\2767804707.py:19) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_13012]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/depthwise_conv2d_3/depthwise:\n model/activation_7/mul (defined at C:\\Users\\rijju\\AppData\\Local\\Temp\\ipykernel_4052\\821000116.py:2)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "modelOri = EfficientNet(input_shape, num_emotions)\n",
    "modelOri.compile(optimizer=Adam(),\n",
    "              loss={'valence': 'mse', 'arousal': 'mse', 'emotion': 'categorical_crossentropy'},\n",
    "              metrics={'valence': ['mae','accuracy',ccc,rmse], 'arousal': ['mae','accuracy', ccc, rmse], 'emotion': ['accuracy']})\n",
    "# input_shape = [224, 224]\n",
    "# Training and validation data generators\n",
    "train_genOri = data_generator(train_dir, train_ann_file, batch_size, input_shape)\n",
    "val_genOri = data_generator(val_dir, val_ann_file, batch_size, input_shape)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='valence_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_modelOri.h5', save_best_only=True, monitor='valence_loss'),\n",
    "    ReduceLROnPlateau(monitor='valence_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "historyOri = modelOri.fit(train_genOri, steps_per_epoch=epoch_steps, epochs=epoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plots/EfficientNethistoryOri.pkl', 'wb') as file:\n",
    "    pickle.dump(historyOri.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect true labels and predictions\n",
    "y_true_valence, y_pred_valence = [], []\n",
    "y_true_arousal, y_pred_arousal = [], []\n",
    "y_true_emotion, y_pred_emotion = [], []\n",
    "\n",
    "resultallOri = modelOri.evaluate(val_genOri, steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "\n",
    "df_results = pd.DataFrame([resultallOri], columns=modelOri.metrics_names)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(\"plots/csv_files_results/EfficientNetOri_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(val_steps):\n",
    "    x_val, y_val = next(val_genOri)\n",
    "    y_pred = modelOri.predict(x_val)\n",
    "    \n",
    "    y_true_valence.extend(y_val['valence'])\n",
    "    y_pred_valence.extend(y_pred[0].squeeze())  # Squeeze to match shape\n",
    "    y_true_arousal.extend(y_val['arousal'])\n",
    "    y_pred_arousal.extend(y_pred[1].squeeze())  # Squeeze to match shape\n",
    "    y_true_emotion.extend(y_val['emotion'])\n",
    "    y_pred_emotion.extend(y_pred[2])\n",
    "\n",
    "# Convert lists to arrays\n",
    "y_true_valence = np.array(y_true_valence)\n",
    "y_pred_valence = np.array(y_pred_valence)\n",
    "y_true_arousal = np.array(y_true_arousal)\n",
    "y_pred_arousal = np.array(y_pred_arousal)\n",
    "y_true_emotion = np.argmax(np.array(y_true_emotion), axis=1)\n",
    "y_pred_emotion = np.argmax(np.array(y_pred_emotion), axis=1)\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score for emotion\n",
    "true_positives = np.sum((y_pred_emotion == y_true_emotion) & (y_true_emotion == 1))\n",
    "predicted_positives = np.sum(y_pred_emotion == 1)\n",
    "actual_positives = np.sum(y_true_emotion == 1)\n",
    "\n",
    "precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + np.finfo(float).eps)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "evaluation_metrics = {\n",
    "    \"Emotion precision\": round(precision, 4),\n",
    "    \"Emotion recall\": round(recall, 4),\n",
    "    \"Emotion F1-Score\": round(f1_score, 4),\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame([evaluation_metrics])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(\"plots/csv_files_results/PRF_emotion_EfficientNetOri.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(historyOri.history['valence_loss'], label='Valence Loss')\n",
    "plt.plot(historyOri.history['arousal_loss'], label='Arousal Loss')\n",
    "plt.plot(historyOri.history['emotion_loss'], label='Emotion Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(historyOri.history['valence_accuracy'], label='Valence Accuracy')\n",
    "plt.plot(historyOri.history['arousal_accuracy'], label='Arousal Accuracy')\n",
    "plt.plot(historyOri.history['emotion_accuracy'], label='Emotion Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/EfficientNetOritrainingloss_acc.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCW = EfficientNet(input_shape, num_emotions)\n",
    "# Calculate class weights\n",
    "total_samples = 134415 + 74874 + 25459 + 24882 + 14090 + 6378 + 3803 + 3750\n",
    "class_weights = {\n",
    "    \"valence\": 1.0,\n",
    "    \"arousal\": 1.0,\n",
    "    \"emotion\": {\n",
    "        0: total_samples / 134415,  # Happy\n",
    "        1: total_samples / 74874,   # Neutral\n",
    "        2: total_samples / 25459,   # Sad\n",
    "        3: total_samples / 24882,   # Anger\n",
    "        4: total_samples / 14090,   # Surprise\n",
    "        5: total_samples / 6378,    # Fear\n",
    "        6: total_samples / 3803,    # Disgust\n",
    "        7: total_samples / 3750     # Contempt\n",
    "    }\n",
    "}\n",
    "\n",
    "modelCW.compile(optimizer=Adam(),\n",
    "              loss={'valence': 'mse', 'arousal': 'mse', 'emotion': 'categorical_crossentropy'},\n",
    "              metrics={'valence': ['mae','accuracy',ccc, rmse], 'arousal': ['mae','accuracy', ccc, rmse], 'emotion': ['accuracy']},\n",
    "              loss_weights=class_weights)  # Pass class weights to the loss_weights parameter\n",
    "\n",
    "# Training and validation data generators\n",
    "train_genCW = data_generator(train_dir, train_ann_file, batch_size, input_shape)\n",
    "val_genCW = data_generator(val_dir, val_ann_file, batch_size, input_shape)\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='valence_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_modelAlexNetWC.h5', save_best_only=True, monitor='valence_loss'),\n",
    "    ReduceLROnPlateau(monitor='valence_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "historyCW = modelCW.fit(train_genCW, steps_per_epoch=epoch_steps, epochs=epoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plots/EfficientNethistoryCW.pkl', 'wb') as file:\n",
    "    pickle.dump(historyCW.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect true labels and predictions\n",
    "y_true_valence, y_pred_valence = [], []\n",
    "y_true_arousal, y_pred_arousal = [], []\n",
    "y_true_emotion, y_pred_emotion = [], []\n",
    "\n",
    "resultallCW = modelCW.evaluate(val_genCW, steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "df_results = pd.DataFrame([resultallCW], columns=modelCW.metrics_names)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(\"plots/csv_files_results/EfficientNetCW_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(val_steps):\n",
    "    x_val, y_val = next(val_genCW)\n",
    "    y_pred = modelCW.predict(x_val)\n",
    "    \n",
    "    y_true_valence.extend(y_val['valence'])\n",
    "    y_pred_valence.extend(y_pred[0].squeeze())  # Squeeze to match shape\n",
    "    y_true_arousal.extend(y_val['arousal'])\n",
    "    y_pred_arousal.extend(y_pred[1].squeeze())  # Squeeze to match shape\n",
    "    y_true_emotion.extend(y_val['emotion'])\n",
    "    y_pred_emotion.extend(y_pred[2])\n",
    "\n",
    "# Convert lists to arrays\n",
    "y_true_valence = np.array(y_true_valence)\n",
    "y_pred_valence = np.array(y_pred_valence)\n",
    "y_true_arousal = np.array(y_true_arousal)\n",
    "y_pred_arousal = np.array(y_pred_arousal)\n",
    "y_true_emotion = np.argmax(np.array(y_true_emotion), axis=1)\n",
    "y_pred_emotion = np.argmax(np.array(y_pred_emotion), axis=1)\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score for emotion\n",
    "true_positives = np.sum((y_pred_emotion == y_true_emotion) & (y_true_emotion == 1))\n",
    "predicted_positives = np.sum(y_pred_emotion == 1)\n",
    "actual_positives = np.sum(y_true_emotion == 1)\n",
    "\n",
    "precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + np.finfo(float).eps)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "evaluation_metrics = {\n",
    "    \"Emotion precision\": round(precision, 4),\n",
    "    \"Emotion recall\": round(recall, 4),\n",
    "    \"Emotion F1-Score\": round(f1_score, 4),\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame([evaluation_metrics])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(\"plots/csv_files_results/PRF_emotion_EfficientNetCW.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(historyCW.history['valence_loss'], label='Valence Loss')\n",
    "plt.plot(historyCW.history['arousal_loss'], label='Arousal Loss')\n",
    "plt.plot(historyCW.history['emotion_loss'], label='Emotion Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(historyCW.history['valence_accuracy'], label='Valence Accuracy')\n",
    "plt.plot(historyCW.history['arousal_accuracy'], label='Arousal Accuracy')\n",
    "plt.plot(historyCW.history['emotion_accuracy'], label='Emotion Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/EfficientNetCWtrainingloss_acc.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "modelUD = EfficientNet(input_shape, num_emotions)\n",
    "modelUD.compile(optimizer=Adam(),\n",
    "              loss={'valence': 'mse', 'arousal': 'mse', 'emotion': 'categorical_crossentropy'},\n",
    "              metrics={'valence': ['mae','accuracy',ccc, rmse], 'arousal': ['mae','accuracy', ccc, rmse], 'emotion': ['accuracy']})\n",
    "\n",
    "# Training and validation data generators\n",
    "train_genUD = data_generator_down_up(train_dir, train_ann_file, batch_size, input_shape, augment=True)\n",
    "val_genUD = data_generator(val_dir, val_ann_file, batch_size, input_shape)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='valence_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_modelOri.h5', save_best_only=True, monitor='valence_loss'),\n",
    "    ReduceLROnPlateau(monitor='valence_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "historyUD = modelUD.fit(train_genUD, steps_per_epoch=epoch_steps, epochs=epoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plots/EfficientNethistoryUD.pkl', 'wb') as file:\n",
    "    pickle.dump(historyUD.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect true labels and predictions\n",
    "y_true_valence, y_pred_valence = [], []\n",
    "y_true_arousal, y_pred_arousal = [], []\n",
    "y_true_emotion, y_pred_emotion = [], []\n",
    "\n",
    "resultallUD = modelUD.evaluate(val_genUD, steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "df_results = pd.DataFrame([resultallUD], columns=modelUD.metrics_names)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(\"plots/csv_file_results/EfficientNetUD_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(val_steps):\n",
    "    x_val, y_val = next(val_genUD)\n",
    "    y_pred = modelUD.predict(x_val)\n",
    "    \n",
    "    y_true_valence.extend(y_val['valence'])\n",
    "    y_pred_valence.extend(y_pred[0].squeeze())  # Squeeze to match shape\n",
    "    y_true_arousal.extend(y_val['arousal'])\n",
    "    y_pred_arousal.extend(y_pred[1].squeeze())  # Squeeze to match shape\n",
    "    y_true_emotion.extend(y_val['emotion'])\n",
    "    y_pred_emotion.extend(y_pred[2])\n",
    "\n",
    "# Convert lists to arrays\n",
    "y_true_valence = np.array(y_true_valence)\n",
    "y_pred_valence = np.array(y_pred_valence)\n",
    "y_true_arousal = np.array(y_true_arousal)\n",
    "y_pred_arousal = np.array(y_pred_arousal)\n",
    "y_true_emotion = np.argmax(np.array(y_true_emotion), axis=1)\n",
    "y_pred_emotion = np.argmax(np.array(y_pred_emotion), axis=1)\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score for emotion\n",
    "true_positives = np.sum((y_pred_emotion == y_true_emotion) & (y_true_emotion == 1))\n",
    "predicted_positives = np.sum(y_pred_emotion == 1)\n",
    "actual_positives = np.sum(y_true_emotion == 1)\n",
    "\n",
    "precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + np.finfo(float).eps)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "evaluation_metrics = {\n",
    "    \"Emotion precision\": round(precision, 4),\n",
    "    \"Emotion recall\": round(recall, 4),\n",
    "    \"Emotion F1-Score\": round(f1_score, 4),\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame([evaluation_metrics])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(\"plots/csv_files_results/PRF_emotion_EfficientNetUD.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(historyUD.history['valence_loss'], label='Valence Loss')\n",
    "plt.plot(historyUD.history['arousal_loss'], label='Arousal Loss')\n",
    "plt.plot(historyUD.history['emotion_loss'], label='Emotion Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(historyUD.history['valence_accuracy'], label='Valence Accuracy')\n",
    "plt.plot(historyUD.history['arousal_accuracy'], label='Arousal Accuracy')\n",
    "plt.plot(historyUD.history['emotion_accuracy'], label='Emotion Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/EfficientNetUDtrainingloss_acc.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('plots/EfficientNethistoryOri.pkl', 'rb') as file:\n",
    "    historyOri = pickle.load(file)\n",
    "with open('plots/EfficientNethistoryCW.pkl', 'rb') as file:\n",
    "    historyCW = pickle.load(file)\n",
    "with open('plots/EfficientNethistoryUD.pkl', 'rb') as file:\n",
    "    historyUD = pickle.load(file)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(historyOri['valence_loss'], label='EfficientNet (Original data)')\n",
    "plt.plot(historyCW['valence_loss'], label='EfficientNet (Class weight)')\n",
    "plt.plot(historyUD['valence_loss'], label='EfficientNet (Up/Down sampling)')\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Valence Loss', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Valence Loss During Training', fontsize=16)\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.95, top=0.9, bottom=0.15)\n",
    "\n",
    "plt.savefig(\"plots/EfficientNet_valenceLoss.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# emotion_mapping = {\n",
    "#     0: 'Neutral',\n",
    "#     1: 'Happy',\n",
    "#     2: 'Sad',\n",
    "#     3: 'Surprise',\n",
    "#     4: 'Fear',\n",
    "#     5: 'Disgust',\n",
    "#     6: 'Anger',\n",
    "#     7: 'Contempt'\n",
    "# }\n",
    "# # Plot confusion matrix for emotion prediction with mapped labels\n",
    "# emotion_labels_mapped = [emotion_mapping[i] for i in range(num_emotions)]\n",
    "# cm_emotion_mapped = confusion_matrix(y_true_emotion, y_pred_emotion)\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(cm_emotion_mapped, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_labels_mapped, yticklabels=emotion_labels_mapped)\n",
    "# # plt.title('Emotion Prediction Confusion Matrix')\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.ylabel('True Label')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
